import streamlit as st
import os
from dotenv import find_dotenv, load_dotenv

from components import stream_text

class Chat:
    def __init__(self):
        """
        Initialise la classe Chat avec la cl√© API n√©cessaire pour utiliser le mod√®le Mistral.
        """
        # R√©cup√©ration de la cl√© API Mistral
        try:
            load_dotenv(find_dotenv())
            self.API_KEY = os.getenv("MISTRAL_API_KEY")
        except FileNotFoundError:
            self.API_KEY = st.secrets["MISTRAL_API_KEY"]
        
        # D√©finition du prompt de r√¥le pour l'IA [TEMP]
        self.role_prompt = ""

    def run(self):
        """
        Lance l'affichage du chat avec l'IA Mistral.
        """
        
        # Avertissement si la cl√© API Mistral n'est pas pr√©sente
        if not self.API_KEY:
            st.error(
                "**Application indisponible :** Vous n'avez pas rajout√© votre cl√© API Mistral dans les fichiers de l'application. "
                "Veuillez ajouter le fichier `.env` √† la racine du projet puis red√©marrer l'application.", icon="‚ö†Ô∏è"
            )
            st.session_state['found_mistral_api'] = False
        else:
            st.session_state['found_mistral_api'] = True

        # V√©rification si l'historique de la conversation est initialis√©
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # Affichage de l'historique de la conversation
        for message in st.session_state.messages:
            # Affichage des messages de l'utilisateur
            if message["role"] == "User":
                with st.chat_message(message["role"], avatar="üë§"):
                    st.write(message["content"])
            # Affichage des messages de l'IA
            elif message["role"] == "assistant":
                with st.chat_message(message["role"], avatar="‚ú®"):
                    st.markdown(message["content"])
                    metrics = message["metrics"]
                    st.markdown(
                        f"üì∂ *Latence : {metrics['latency']:.2f} secondes* | "
                        f"üí≤ *Co√ªt : {metrics['euro_cost']:.6f} ‚Ç¨* | "
                        f"‚ö° *Utilisation √©nerg√©tique : {metrics['energy_usage']} kWh* | "
                        f"üå°Ô∏è *Potentiel de r√©chauffement global : {metrics['gwp']} kgCO2eq*"
                    )

        # Zone de saisie pour le chat avec l'IA [TEMP]
        if message := st.chat_input(
            placeholder="√âcrivez votre message", key="search_restaurant_temp", 
            disabled=not st.session_state.get('found_mistral_api', False)
        ):
            if message.strip():

                # Affichage du nouveau message de l'utilisateur
                with st.chat_message("user", avatar="üë§"):
                    st.write(message)

                # Ajout du message √† l'historique de la conversation
                st.session_state.messages.append({"role": "User", "content": message})

                # Initialisation des connaissances de l'IA
                if 'bdd_chunks' not in st.session_state:
                    with st.spinner("D√©marrage de l'IA..."):
                        st.session_state['bdd_chunks'] = instantiate_bdd()

                if 'llm' not in st.session_state:
                    st.session_state['llm'] = AugmentedRAG(
                        role_prompt=self.role_prompt,
                        generation_model="mistral-large-latest",
                        bdd_chunks=st.session_state['bdd_chunks'],
                        top_n=3,
                        max_tokens=3000,
                        temperature=0.3,
                    )

                # R√©cup√©ration de la r√©ponse de l'IA
                llm = st.session_state['llm']
                response = llm(
                    query=message,
                    history=st.session_state.messages,
                )

                # Affichage de la r√©ponse de l'IA
                with st.chat_message("AI", avatar="‚ú®"):
                    st.write_stream(stream_text(response["response"]))
                    st.markdown(
                        f"üì∂ *Latence : {response['latency']:.2f} secondes* | "
                        f"üí≤ *Co√ªt : {response['euro_cost']:.6f} ‚Ç¨* | "
                        f"‚ö° *Utilisation √©nerg√©tique : {response['energy_usage']} kWh* | "
                        f"üå°Ô∏è *Potentiel de r√©chauffement global : {response['gwp']} kgCO2eq*"
                    )

                # Ajout de la r√©ponse de l'IA √† l'historique de la conversation
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response["response"],
                    "metrics": {
                        "latency": response["latency"],
                        "euro_cost": response["euro_cost"],
                        "energy_usage": response["energy_usage"],
                        "gwp": response["gwp"]
                    }
                })