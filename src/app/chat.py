"""
Ce fichier d√©finit la classe Chat pour g√©rer les interractions avec l'IA.
"""

import os
import asyncio
import streamlit as st
import PyPDF2

from src.app.components import stream_text
from src.pipeline import EnhancedLLMSecurityManager
from src.rag.model_api import MultiModelLLM

class Chat:
    """
    Classe pour g√©rer les interractions avec l'IA.
    """

    def __init__(self, selected_chat: str, initial_question: str = None):
        """
        Initialise la classe Chat avec le chat s√©lectionn√© et la question initiale si disponible.

        Args:
            selected_chat (str): Chat s√©lectionn√© pour la conversation.
            initial_question (str, optionnel): Question initiale √† poser √† l'IA. None par d√©faut.
        """

        # R√©cup√©ration du chat s√©lectionn√©
        self.selected_chat = selected_chat

        # Si ce ne sont pas les suggestions
        if selected_chat != "suggestions":
            # Initialisation des messages du chat
            if "chats" not in st.session_state:
                st.session_state["chats"] = {}

            # V√©rification si le chat s√©lectionn√© existe
            if self.selected_chat not in st.session_state["chats"]:
                st.session_state["chats"][self.selected_chat] = []

            # Stockage de la question initiale
            self.initial_question = initial_question
            st.session_state["initial_question"] = None

            # Mise en page du chat avec l'IA
            self.header_container = st.container()
            self.chat_container = self.header_container.container(height=500)

        # Si les cl√©s d'API sont trouv√©es
        if st.session_state["found_api_keys"] is True:
            # Initialisation du LLM
            if "llm" not in st.session_state:
                st.session_state["llm"] = MultiModelLLM(
                    api_key_mistral=os.getenv("MISTRAL_API_KEY"),
                    api_key_gemini=os.getenv("GEMINI_API_KEY")
                )
            self.llm = st.session_state["llm"]
        # Si les cl√©s d'API ne sont pas trouv√©es
        else:
            with self.chat_container.chat_message("", avatar="‚ö†Ô∏è"):
                st.write(
                    "**Conversation avec l'IA indisponible :** "
                    "Une ou plusieurs cl√©s d'API sont introuvables."
                )

    def get_suggested_questions(self) -> list:
        """
        G√©n√®re 5 exemples de question avec l'IA.

        Returns:
        """
        prompt = (
            "Tu es une intelligence artificielle sp√©cialis√©e dans l'aide aux √©l√®ves √† l'√©cole. "
            "G√©n√®re 5 questions courtes dans diff√©rentes mati√®res sans les pr√©ciser, "
            "qu'un √©l√®ve pourrait te poser sur une notion de cours. "
            "R√©pond uniquement en donnant les 5 questions sous forme de liste de tirets, "
            "sans explication suppl√©mentaire."
        )
        response = asyncio.run(self.llm.generate(prompt=prompt, model="mistral-large-latest"))
        questions = response["response"].split('\n')
        return [q.strip("- ").strip() for q in questions[:5]]

    def generate_chat_name(self, initial_message: str):
        """
        G√©n√®re un nom pour la conversation en fonction du premier message avec l'IA.

        Args:
            initial_message (str): Message initial de la conversation.
        """
        for _ in range(5):
            prompt = (
                "Tu es une intelligence artificielle sp√©cialis√©e "
                "dans la cr√©ation de nom de conversation. "
                "En te basant sur le texte suivant, qui est le premier message de la conversation, "
                "propose un nom d'un maximum de 30 caract√®res pour cette conversation. "
                "R√©pond uniquement en donnant le nom de la conversation "
                "sans explication suppl√©mentaire. "
                f"Voici le texte : {initial_message}"
            )
            response = asyncio.run(self.llm.generate(prompt=prompt, model="mistral-large-latest"))
            generated_name = response["response"].strip()

            if len(generated_name) > 30:
                continue
            if generated_name in st.session_state["chats"]:
                continue

            st.session_state["chats"][generated_name] = st.session_state["chats"].pop(
                self.selected_chat
            )
            st.session_state["selected_chat"] = generated_name
            self.selected_chat = generated_name
            st.rerun()

    def run(self):
        """
        Lance l'affichage du chat avec l'IA.
        """

        # Affichage de l'historique de la conversation
        for message in st.session_state["chats"][self.selected_chat]:
            # Affichage des messages de l'utilisateur
            if message["role"] == "User":
                with self.chat_container.chat_message(message["role"], avatar="üë§"):
                    st.write(message["content"])

            # Affichage des messages de l'IA
            elif message["role"] == "AI":
                with self.chat_container.chat_message(message["role"], avatar="‚ú®"):
                    st.markdown(message["content"])
                    metrics = message["metrics"]
                    st.markdown(
                        f"üì∂ *Latence : {metrics['latency']:.2f} secondes* | "
                        f"üí≤ *Co√ªt : {metrics['euro_cost']:.6f} ‚Ç¨* | "
                        f"‚ö° *Utilisation √©nerg√©tique : {metrics['energy_usage']} kWh* | "
                        f"üå°Ô∏è *Potentiel de r√©chauffement global : {metrics['gwp']} kgCO2eq*"
                    )

            # Affichage des messages de s√©curit√©
            elif message["role"] == "Guardian":
                with self.chat_container.chat_message(message["role"], avatar="‚õî"):
                    st.write(message["content"])

        # Si une question initiale est pr√©sente, l'envoyer automatiquement
        if self.initial_question and not st.session_state["chats"][self.selected_chat]:
            self.handle_user_message(self.initial_question)

        # Mise en page de l'interraction avec l'IA
        cols = self.header_container.columns([3, 10, 1])

        # Choix du mod√®le [TEMP]
        with cols[0]:
            st.selectbox(
                "NULL",
                label_visibility="collapsed",
                options=["Option 1", "Option 2", "Option 3"],
                index=0,
                disabled=not st.session_state.get("found_api_keys", False),
            )

        # Zone de saisie pour le chat avec l'IA
        with cols[1]:
            if message := st.chat_input(
                placeholder="√âcrivez votre message",
                key=f"chat_input_{self.selected_chat}",
                disabled=not st.session_state.get("found_api_keys", False),
            ):
                if message.strip():
                    self.handle_user_message(message)

        # Bouton pour ajouter un fichier PDF [TEMP]
        with cols[2]:
            if st.button(
                "",
                icon=":material/attach_file:",
                disabled=not st.session_state.get("found_api_keys", False),
            ):
                self.upload_files_dialog()

    def handle_user_message(self, message: str):
        """
        G√®re le message de l'utilisateur et envoie une requ√™te √† l'IA.

        Args:
            message (str): Message de l'utilisateur.
        """

        # Affichage du nouveau message de l'utilisateur
        with self.chat_container.chat_message("User", avatar="üë§"):
            st.write(message)

        # Ajout du message √† l'historique de la conversation
        st.session_state["chats"][self.selected_chat].append(
            {"role": "User", "content": message}
        )

        # Initialisation du pipeline de s√©curit√©
        security_manager = EnhancedLLMSecurityManager(message)

        # Validation du message de l'utilisateur
        is_valid_message = security_manager.validate_input()

        # Si le message de utilisateur est autoris√©
        if is_valid_message is True:
            # Envoi du message et r√©cup√©ration de la r√©ponse de l'IA
            response = asyncio.run(self.llm.generate(prompt=message, model="mistral-large-latest"))

            # Affichage de la r√©ponse de l'IA
            with self.chat_container.chat_message("AI", avatar="‚ú®"):
                st.write_stream(stream_text(response["response"]))
                st.markdown(
                    f"üì∂ *Latence : {response['latency']:.2f} secondes* | "
                    f"üí≤ *Co√ªt : {response['euro_cost']:.6f} ‚Ç¨* | "
                    f"‚ö° *Utilisation √©nerg√©tique : {response['energy_usage']} kWh* | "
                    f"üå°Ô∏è *Potentiel de r√©chauffement global : {response['gwp']} kgCO2eq*"
                )

            # Ajout de la r√©ponse de l'IA √† l'historique de la conversation
            st.session_state["chats"][self.selected_chat].append(
                {
                    "role": "AI",
                    "content": response["response"],
                    "metrics": {
                        "latency": response["latency"],
                        "euro_cost": response["euro_cost"],
                        "energy_usage": response["energy_usage"],
                        "gwp": response["gwp"],
                    },
                }
            )
        else:
            # D√©finition du message de s√©curit√©
            message = (
                "Votre message n'a pas √©t√© trait√© pour des raisons de s√©curit√©. "
                "Veuillez reformuler votre message."
            )

            # Affichage du message de s√©curit√©
            with self.chat_container.chat_message("Guardian", avatar="‚õî"):
                st.write_stream(stream_text(message))

            # Ajout du message de s√©curit√© √† l'historique de la conversation
            st.session_state["chats"][self.selected_chat].append(
                {
                    "role": "Guardian",
                    "content": message,
                }
            )

        # Si c'est le premier message envoy√©, alors g√©n√©ration du nom de la conversation
        if len(st.session_state["chats"][self.selected_chat]) == 2:
            print(self.selected_chat)
            self.generate_chat_name(message)

    @st.dialog("Ajouter des fichiers PDF")
    def upload_files_dialog(self):
        """
        Ouvre une bo√Æte de dialogue pour ajouter des fichiers PDF.
        """

        # Espace pour ajouter des fichiers
        uploaded_files = st.file_uploader(
            "NULL",
            label_visibility="collapsed",
            type=["pdf"],
            accept_multiple_files=True,
        )

        # Bouton pour ajouter et traiter les fichiers s√©lectionn√©s
        if st.button(
            "Ajouter les fichiers s√©lectionn√©s",
            icon=":material/upload_file:",
            disabled=not uploaded_files,
        ):
            with st.status(
                "**Ajout de(s) fichier(s) en cours... Ne fermez pas la fen√™tre !**",
                expanded=True,
            ) as status:
                # Lecture du contenu de chaque fichier PDF
                documents = {}
                for file in uploaded_files:
                    st.write(f"Ajout du fichier {file.name}...")
                    pdf_reader = PyPDF2.PdfReader(file)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text()
                    documents[file.name] = text
                status.update(
                    label="**Les fichiers ont √©t√© ajout√©s avec succ√®s ! "
                    "Vous pouvez maintenant fermer la fen√™tre.**",
                    state="complete",
                    expanded=False,
                )
