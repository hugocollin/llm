import streamlit as st
import PyPDF2

from src.app.components import stream_text

class Chat:
    def __init__(self, selected_chat : str, initial_question : str = None):
        """
        Initialise la classe Chat avec le chat s√©lectionn√© et la question initiale si disponible.

        Args:
            selected_chat (str): Chat s√©lectionn√© pour la conversation.
            initial_question (str, optionnel): Question initiale √† poser √† l'IA. None par d√©faut.
        """
        
        # R√©cup√©ration du chat s√©lectionn√©
        self.selected_chat = selected_chat

        # Initialisation des messages du chat
        if "chats" not in st.session_state:
            st.session_state["chats"] = {}

        # V√©rification si le chat s√©lectionn√© existe
        if self.selected_chat not in st.session_state["chats"]:
            st.session_state["chats"][self.selected_chat] = []
        
        # D√©finition du prompt de r√¥le pour l'IA [TEMP]
        self.role_prompt = ""

        # Stockage de la question initiale
        self.initial_question = initial_question
        st.session_state['initial_question'] = None

    def run(self):
        """
        Lance l'affichage du chat avec l'IA Mistral.
        """
        
        # Avertissement si la cl√© API Mistral n'est pas pr√©sente
        if st.session_state['found_mistral_api'] == False:
            # Affichage d'un message d'erreur
            st.error("**Conversation avec l'IA indisponible :** Votre cl√© d'API Mistral est introuvable.", icon=":material/error:")

        # Mise en page du chat avec l'IA
        header_container = st.container()
        self.chat_container = header_container.container(height=500)

        # Affichage de l'historique de la conversation
        for message in st.session_state["chats"][self.selected_chat]:
            # Affichage des messages de l'utilisateur
            if message["role"] == "User":
                with self.chat_container.chat_message(message["role"], avatar="üë§"):
                    st.write(message["content"])

            # Affichage des messages de l'IA
            elif message["role"] == "AI":
                with self.chat_container.chat_message(message["role"], avatar="‚ú®"):
                    st.markdown(message["content"])
                    metrics = message["metrics"]
                    st.markdown(
                        f"üì∂ *Latence : {metrics['latency']:.2f} secondes* | "
                        f"üí≤ *Co√ªt : {metrics['euro_cost']:.6f} ‚Ç¨* | "
                        f"‚ö° *Utilisation √©nerg√©tique : {metrics['energy_usage']} kWh* | "
                        f"üå°Ô∏è *Potentiel de r√©chauffement global : {metrics['gwp']} kgCO2eq*"
                    )
        # Si une question initiale est pr√©sente, l'envoyer automatiquement
        if self.initial_question and not st.session_state["chats"][self.selected_chat]:
            self.handle_user_message(self.initial_question)

        # Mise en page de l'interraction avec l'IA
        cols = header_container.columns([3, 10, 1])

        # Choix du mod√®le [TEMP]
        with cols[0]:
            st.selectbox("NULL", label_visibility="collapsed", options=["Option 1", "Option 2", "Option 3"], index=0, disabled=not st.session_state.get('found_mistral_api', False))

        # Zone de saisie pour le chat avec l'IA [TEMP]
        with cols[1]:
            if message := st.chat_input(placeholder="√âcrivez votre message", key=f"chat_input_{self.selected_chat}", disabled=not st.session_state.get('found_mistral_api', False)):
                if message.strip():
                    self.handle_user_message(message)

        # Bouton pour ajouter un fichier
        with cols[2]:
            if st.button("", icon=":material/attach_file:", disabled=not st.session_state.get('found_mistral_api', False)):
                self.upload_files_dialog()

    def handle_user_message(self, message: str):
        """
        G√®re le message de l'utilisateur et envoie une requ√™te √† l'IA Mistral.

        Args:
            message (str): Message de l'utilisateur.
        """
        
        # Affichage du nouveau message de l'utilisateur
        with self.chat_container.chat_message("User", avatar="üë§"):
            st.write(message)

        # Ajout du message √† l'historique de la conversation
        st.session_state["chats"][self.selected_chat].append({"role": "User", "content": message})

        # # Initialisation des connaissances de l'IA
        # if 'bdd_chunks' not in st.session_state:
        #     with st.spinner("D√©marrage de l'IA..."):
        #         st.session_state['bdd_chunks'] = instantiate_bdd()

        # if 'llm' not in st.session_state:
        #     st.session_state['llm'] = AugmentedRAG(
        #         role_prompt=self.role_prompt,
        #         generation_model="mistral-large-latest",
        #         bdd_chunks=st.session_state['bdd_chunks'],
        #         top_n=3,
        #         max_tokens=3000,
        #         temperature=0.3,
        #     )

        # # R√©cup√©ration de la r√©ponse de l'IA
        # llm = st.session_state['llm']
        # response = llm(
        #     query=message,
        #     history=st.session_state["chats"][self.selected_chat]
        # )

        # Affichage d'un faux message temporaire
        response = {
            "response": "Je ne suis pas encore pr√™t √† r√©pondre √† vos questions, mais je le serai bient√¥t !",
            "latency": 0,
            "euro_cost": 0,
            "energy_usage": 0,
            "gwp": 0
        }

        # Affichage de la r√©ponse de l'IA
        with self.chat_container.chat_message("AI", avatar="‚ú®"):
            st.write_stream(stream_text(response["response"]))
            st.markdown(
                f"üì∂ *Latence : {response['latency']:.2f} secondes* | "
                f"üí≤ *Co√ªt : {response['euro_cost']:.6f} ‚Ç¨* | "
                f"‚ö° *Utilisation √©nerg√©tique : {response['energy_usage']} kWh* | "
                f"üå°Ô∏è *Potentiel de r√©chauffement global : {response['gwp']} kgCO2eq*"
            )

        # Ajout de la r√©ponse de l'IA √† l'historique de la conversation
        st.session_state["chats"][self.selected_chat].append({
            "role": "AI",
            "content": response["response"],
            "metrics": {
                "latency": response['latency'],
                "euro_cost": response['euro_cost'],
                "energy_usage": response['energy_usage'],
                "gwp": response['gwp']
            }
        })

    @st.dialog("Ajouter des fichiers PDF")
    def upload_files_dialog(self):
        """
        Ouvre une bo√Æte de dialogue pour ajouter des fichiers PDF.
        """

        # Affichage de l'espace pour ajouter des fichiers
        uploaded_files = st.file_uploader("NULL", label_visibility="collapsed", type=['pdf'], accept_multiple_files=True)
        
        # Bouton pour ajouter et traiter les fichiers s√©lectionn√©s
        if st.button("Ajouter les fichiers s√©lectionn√©s", icon=":material/upload_file:", disabled=not uploaded_files):
            with st.status("**Ajout de(s) fichier(s) en cours... Ne fermez pas la fen√™tre !**", expanded=True) as status:
                # Lecture du contenu de chaque fichier PDF
                documents = {}
                for file in uploaded_files:
                    st.write(f"Ajout du fichier {file.name}...")
                    pdf_reader = PyPDF2.PdfReader(file)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text()
                    documents[file.name] = text
                status.update(label="**Les fichiers ont √©t√© ajout√©s avec succ√®s ! Vous pouvez maintenant fermer la fen√™tre.**", state="complete", expanded=False)