{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Evidya\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        accuracy  precision    recall  f1 score\n",
      "Naive Bayes             0.870690   0.868852  0.883333  0.876033\n",
      "Logistic Regression     0.974138   1.000000  0.950000  0.974359\n",
      "Support Vector Machine  0.948276   1.000000  0.900000  0.947368\n",
      "Random Forest           0.887931   1.000000  0.783333  0.878505\n",
      "Best Model: Logistic Regression, Metrics: {'accuracy': 0.9741379310344828, 'precision': 1.0, 'recall': 0.95, 'f1 score': 0.9743589743589743}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from promptClassifier import PromptClassifier\n",
    "\n",
    "prompt_classifier = PromptClassifier()\n",
    "\n",
    "# Load the training and test data from the json files\n",
    "# train_json = os.path.join(os.path.dirname(__file__), 'guardrail_dataset_train.json')\n",
    "\n",
    "prompt_classifier.load_train_and_test_data_from_json('guardrail_dataset_train.json', 'guardrail_dataset_test.json')\n",
    "results = prompt_classifier.train_and_evaluate()\n",
    "print(results)\n",
    "best_model_name, best_metrics = prompt_classifier.get_best_model()\n",
    "print(f\"Best Model: {best_model_name}, Metrics: {best_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le meilleur modèle a été exporté avec succès vers best_prompt_model.pkl.\n"
     ]
    }
   ],
   "source": [
    "prompt_classifier.export_best_model(\"best_prompt_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"best_prompt_model.pkl\", 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Exemple de prédiction avec le modèle chargé\n",
    "new_prompts = [\"Explain relativity.\", \"' OR 1=1; DROP TABLE users; --\"]\n",
    "new_embeddings = [prompt_classifier.get_bert_embedding(prompt) for prompt in new_prompts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.predict(new_embeddings)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
